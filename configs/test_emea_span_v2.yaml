# EMEA Span-based NER v2 with nlstruct-inspired hyperparameters
# Target: exact_f1 > 60% (currently ~43%)
# Key changes from previous config:
#   - use_crf: true (real CRF marginals instead of softmax)
#   - negative_ratio: -1 (use all spans, no sampling)
#   - bert_learning_rate: 5e-5 (separate from other layers)
#   - gradient_clip: 10.0
#   - max_steps: 4000 (instead of epochs)

task: ner_span
dataset: emea
model: bert_span_ner
model_name_or_path: /lustre/fswork/projects/rech/rua/uvb79kr/DrBenchmark/models/almanach_camembert-bio-base
do_train: true
do_eval: true

backend_args:
  # Span enumeration
  max_span_length: 40
  negative_ratio: -1  # Use all spans (nlstruct approach)
  max_length: 256
  max_spans: 512
  # BiLSTM contextualizer (nlstruct defaults)
  lstm_hidden_size: 400
  lstm_num_layers: 3
  # Biaffine scorer
  biaffine_hidden_size: 64
  # Combined scoring
  use_tagger: true
  use_biaffine: true
  use_crf: true  # Real CRF marginals
  tagger_weight: 1.0
  biaffine_weight: 1.0
  dropout: 0.4

training:
  output_dir: /lustre/fswork/projects/rech/rua/uvb79kr/tabib/runs/test_emea_span_v2
  max_steps: 4000  # nlstruct default
  per_device_train_batch_size: 16
  per_device_eval_batch_size: 16
  learning_rate: 0.001  # For BiLSTM/CRF/Biaffine layers
  bert_learning_rate: 0.00005  # For BERT encoder (lower)
  warmup_ratio: 0.1
  gradient_clip: 10.0  # Gradient clipping
  weight_decay: 0.01
  logging_steps: 50
  eval_strategy: steps
  eval_steps: 500
  save_strategy: steps
  save_steps: 500
  save_total_limit: 2
  load_best_model_at_end: true
  metric_for_best_model: eval_f1
  greater_is_better: true
  seed: 42
  bf16: true

predict_args:
  filter_predictions: no_overlapping_same_label
